{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import requests\n",
    "from dominate import document \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_links(): \n",
    "    '''\n",
    "    go through the csv file and find the column that contains the chatgpt links \n",
    "\n",
    "    and apply link scraper to it \n",
    "    '''\n",
    "\n",
    "    data = pd.read_csv(\"../raw/datatset.csv\")\n",
    "\n",
    "    data.column.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def link_scraper(link, number):\n",
    "    \"\"\"\n",
    "    given link, creates a beautiful soup object and dump content into an html file \n",
    "\n",
    "    link: chatgpt link that requires scraping \n",
    "    number: the order that they appear in the field where folks put in chat links on the qualtrics results file \n",
    "    \"\"\"\n",
    "    #  if link is empty, just use this dummy one\n",
    "    if link == \"\": \n",
    "        link = \"https://chatgpt.com/share/68cc3ef7-5afc-8012-8361-c257e814f8d5\"\n",
    "\n",
    "    response = requests.get(link)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    with open(f\"data/raw/raw_output.html{number}\", \"w\", encoding = \"UTF-8\") as file: \n",
    "        file.write(str(soup.prettify()))\n",
    "\n",
    "    print(\"Done parsing links\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapes link\n",
    "link = input(\"Give me a link, Agent Cooper: \")\n",
    "link_scraper(link, 1)\n",
    "\n",
    "# TODO: sub w a more generalised version \n",
    "# process_info(\"data/raw/raw_output.html\")\n",
    "\n",
    "find_title(\"data/raw/raw_output.html\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
