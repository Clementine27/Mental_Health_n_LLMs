{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "import re \n",
    "# import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# from pathlib import Path\n",
    "# script_location = Path(__file__).absolute().parent\n",
    "# target_directory = script_location / 'mentalLLM' / 'config.py'\n",
    "\n",
    "sys.path.append(\"/Users/ngokykhanhthu/Documents/Mental_Health_n_LLMs\")\n",
    "from mentalLLM.config import RAW_DIR, INTERIM_DIR, PROCESSED_DIR, FINAL_DIR, DATASET, DATA_KEYWORD, CHAT_GPT_QUESTION\n",
    "\n",
    "from chat_gpt_scraper import remove_white_spaces, is_unix_time, convert_unix_time, find_links, scrape_link, retrieve_the_json_portion, extract_json_object, create_csv_files, extract_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_chat_message(unix_stamp, data): \n",
    "    \"\"\"\n",
    "    helper method for create_csv_files\n",
    "    given unix stamp and the json list, figure out if it is gibberish or chat message\n",
    "   \n",
    "    Args:\n",
    "        unix_stamp (float): an unix timestamp \n",
    "        data (list): the json file \n",
    "\n",
    "    Returns: \n",
    "        True if it is a valid chat message\n",
    "        False otherwise  \n",
    "    \"\"\"\n",
    "    # TODO \n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_msg(unix_stamp, data): \n",
    "    \"\"\"\n",
    "    helper method for create_csv_files\n",
    "    given unix stamp and the json list, extract chat message\n",
    "    Args:\n",
    "        unix_stamp (float): an unix timestamp \n",
    "        data (list): the json file \n",
    "\n",
    "    Returns: \n",
    "        message\n",
    "    \"\"\"\n",
    "    curr_index = data.index(unix_stamp) + 1\n",
    "    strings = []\n",
    "    \n",
    "    entered_string = False \n",
    "    while curr_index < len(data):\n",
    "        curr_element = data[curr_index]\n",
    "        if isinstance(curr_element, str):\n",
    "            strings.append(curr_element)  \n",
    "            curr_index += 1\n",
    "        elif not strings: \n",
    "            curr_index += 1 \n",
    "        else: \n",
    "            curr_index = len(data)\n",
    "\n",
    "    return repr(str(strings[-1]))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sender(message): \n",
    "    \"\"\"\n",
    "    helper method for create_csv_files\n",
    "    given unix stamp and the json list, figure out who sent message \n",
    "     Args:\n",
    "        message (str): a message extracted \n",
    "\n",
    "    Returns: \n",
    "        str: chatgpt/user \n",
    "    \"\"\"\n",
    "    msg_raw = message.strip('\\'')\n",
    "    print(\"messsage before stripping: \", message)\n",
    "\n",
    "    print(\"message after stripping: \", msg_raw)\n",
    "\n",
    "    if re.match(r\"^'\", msg_raw):\n",
    "        return \"chatgpt\" \n",
    "\n",
    "    return \"user\"\n",
    "    # return bool(re.match(r'.*(\"\\\"\"|\"\\'\").*', message[0]))\n",
    "    # message_raw = repr(message)[1:]\n",
    "    # state =  bool(re.fullmatch(r'.*[\\']\".*', message_raw))\n",
    "    # if state: \n",
    "    #     return \"chatgpt\"\n",
    "    # return \"user\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_files(file_path): \n",
    "    \"\"\"\n",
    "    given json file path n order of file,\n",
    "    extracts important information and outputs csv file\n",
    "    \n",
    "    Args:\n",
    "        file_path (os.path): path to json file \n",
    "        order_of_file (int): how to number the resulting file \n",
    "  \n",
    "    \"\"\"\n",
    "\n",
    "    # load json file \n",
    "    with open(file_path, \"r\", encoding='utf-8', errors='strict') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # initialise df \n",
    "    rows = []\n",
    "    \n",
    "    # meta data stuff \n",
    "    title = \"\"\n",
    "    create_time = 0 \n",
    "    update_time = 0\n",
    "\n",
    "    # go through the json list \n",
    "    for i in range(0,len(data)-1, 1): \n",
    "        sender = \"\"\n",
    "        # extract title \n",
    "        if isinstance(data[i], str) and \"title\" in data[i]: \n",
    "            title = data[i+1]\n",
    "            continue\n",
    "\n",
    "        if not is_unix_time(data[i]): \n",
    "            continue \n",
    "\n",
    "        timestamp = data[i]\n",
    "\n",
    "        # extract file create time \n",
    "        if create_time == 0 : \n",
    "            create_time = convert_unix_time(timestamp)\n",
    "            continue \n",
    "            \n",
    "        # extract file latest update time \n",
    "        elif update_time == 0:     \n",
    "            update_time = convert_unix_time(timestamp)\n",
    "            continue \n",
    "    \n",
    "        # extract timestamp + message \n",
    "        \n",
    "        # if there is a timestamp right afterwards, probs a chatgpt response \n",
    "        elif i + 1 < len(data) and is_unix_time(data[i+1]): \n",
    "            # print(\"data at \", i, \"is \", data[i], \"anddd second cond state is \", is_unix_time(data[i+1]))\n",
    "\n",
    "            timestamp =  data[i+1]\n",
    "            sender = \"chatgpt\"\n",
    "            \n",
    "        # deal w the extra timestamp\n",
    "        elif  i - 1  > 0 and is_unix_time(data[i-1]) : \n",
    "            continue\n",
    "\n",
    "        # if there is not, check if gibberish. \n",
    "        elif not check_if_chat_message(timestamp, data): \n",
    "            continue\n",
    "\n",
    "        # if not, get the message \n",
    "        message = extract_msg(timestamp, data) \n",
    "\n",
    "        if 'sender' != \"\": \n",
    "            sender = classify_sender(message)\n",
    "\n",
    "        # update rows \n",
    "        rows.append([title, \n",
    "                        create_time, \n",
    "                        update_time,\n",
    "                        sender, \n",
    "                        convert_unix_time(timestamp), \n",
    "                        message])\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns=['title', 'create_time', \"update_time\", 'sender', 'timestamp', 'message'])\n",
    "    df.to_csv(f\"../extracted_chat.csv\", quotechar = \"'\", index=False, encoding='utf-8')\n",
    "    # df.to_csv(f\"{FINAL_DIR}/{order_of_file}_extracted_chat.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messsage before stripping:  'how to build an LLM model from scratch? '\n",
      "message after stripping:  how to build an LLM model from scratch? \n",
      "messsage before stripping:  'parent_id'\n",
      "message after stripping:  parent_id\n",
      "messsage before stripping:  'model_set_context'\n",
      "message after stripping:  model_set_context\n",
      "messsage before stripping:  'thoughts'\n",
      "message after stripping:  thoughts\n",
      "messsage before stripping:  '{search_query: [{q:how to train a large language model from scratch guide 2024 2025}, {q:state of the art LLM training libraries DeepSpeed Megatron-DeepSpeed FlashAttention 2024}, {q:compute and data requirements to train a 7B 70B parameter model 2024 blog}, {q:tokenizers Byte-Pair Encoding SentencePiece WordPiece tokenization for LLMs 2024}, {q:Hugging Face transformers training from scratch tutorial 2024}], response_length: short}'\n",
      "message after stripping:  {search_query: [{q:how to train a large language model from scratch guide 2024 2025}, {q:state of the art LLM training libraries DeepSpeed Megatron-DeepSpeed FlashAttention 2024}, {q:compute and data requirements to train a 7B 70B parameter model 2024 blog}, {q:tokenizers Byte-Pair Encoding SentencePiece WordPiece tokenization for LLMs 2024}, {q:Hugging Face transformers training from scratch tutorial 2024}], response_length: short}\n",
      "messsage before stripping:  'The output of this plugin was redacted.'\n",
      "message after stripping:  The output of this plugin was redacted.\n",
      "messsage before stripping:  '54344faa-c39d-48b4-aa9f-fcd60efb334f'\n",
      "message after stripping:  54344faa-c39d-48b4-aa9f-fcd60efb334f\n",
      "messsage before stripping:  'I’ll put together a guide for building an LLM from scratch, starting with design choices, tokenization, architecture, training infrastructure, and more. I need to focus on compute estimates, evaluation, and safety, while citing the top 5 most important sources. I’ll also include practical code/libraries and their associated citations.'\n",
      "message after stripping:  I’ll put together a guide for building an LLM from scratch, starting with design choices, tokenization, architecture, training infrastructure, and more. I need to focus on compute estimates, evaluation, and safety, while citing the top 5 most important sources. I’ll also include practical code/libraries and their associated citations.\n",
      "messsage before stripping:  'Thought for 21s'\n",
      "message after stripping:  Thought for 21s\n",
      "messsage before stripping:  'Nice — building an LLM from scratch is a big, fun engineering + research project. Below I’ll give you a clear, practical end-to-end blueprint: design choices, data & tokenization, model architecture, training infra & libraries, compute/data sizing (with simple formulas), training recipe & hyperparams, evaluation, alignment/fine-tuning, and next steps + reading. I’ll include concrete recommendations you can follow whether you want a small experimental model (100M–1B params) or a production-level 7B+ model.\\\\n\\\\n---\\\\n\\\\n# Quick TL;DR (pick your path)\\\\n- **Toy / learning build (0.1M–1B params):** train on a few GBs of cleaned text; use Hugging Face Transformers + Tokenizers + Datasets; single multi-GPU or a small TPU pod. Good to learn internals.\\\\n- **Medium (1B–7B params):** need multi-GPU training, ZeRO/DeepSpeed or Megatron-DeepSpeed for memory scaling, 10s–100s of billions of tokens. Use mixed precision (bf16/fp16).\\\\n- **Large (7B–70B+ params):** require distributed training (3D parallelism), lots of compute and high quality data (100B+ tokens for compute-optimal training), production infra, careful evaluation & alignment. Use Megatron-DeepSpeed / DeepSpeed / FlashAttention for performance. \\ue200cite\\ue202turn0search1\\ue202turn0search5\\ue201\\\\n\\\\n---\\\\n\\\\n# 1) Decide scope & constraints\\\\n- Choose **target parameter count** (e.g., 125M, 1.3B, 7B, 13B, 70B).\\\\n- Pick a **use case** (chat assistant, code model, domain-specific).\\\\n- Fix **resource limits**: GPUs (type & count), budget, timeline.\\\\nThese choices determine dataset size, model depth/width, and whether you need model/pipeline parallelism.\\\\n\\\\n---\\\\n\\\\n# 2) Data (collection, cleaning, and size)\\\\n- **Sources:** web crawl (CommonCrawl), books, Wikipedia, code repos (if you want code), high-quality curated corpora. Filter for copyright & privacy concerns.\\\\n- **Cleaning:** dedupe,'\n",
      "message after stripping:  Nice — building an LLM from scratch is a big, fun engineering + research project. Below I’ll give you a clear, practical end-to-end blueprint: design choices, data & tokenization, model architecture, training infra & libraries, compute/data sizing (with simple formulas), training recipe & hyperparams, evaluation, alignment/fine-tuning, and next steps + reading. I’ll include concrete recommendations you can follow whether you want a small experimental model (100M–1B params) or a production-level 7B+ model.\\\\n\\\\n---\\\\n\\\\n# Quick TL;DR (pick your path)\\\\n- **Toy / learning build (0.1M–1B params):** train on a few GBs of cleaned text; use Hugging Face Transformers + Tokenizers + Datasets; single multi-GPU or a small TPU pod. Good to learn internals.\\\\n- **Medium (1B–7B params):** need multi-GPU training, ZeRO/DeepSpeed or Megatron-DeepSpeed for memory scaling, 10s–100s of billions of tokens. Use mixed precision (bf16/fp16).\\\\n- **Large (7B–70B+ params):** require distributed training (3D parallelism), lots of compute and high quality data (100B+ tokens for compute-optimal training), production infra, careful evaluation & alignment. Use Megatron-DeepSpeed / DeepSpeed / FlashAttention for performance. \\ue200cite\\ue202turn0search1\\ue202turn0search5\\ue201\\\\n\\\\n---\\\\n\\\\n# 1) Decide scope & constraints\\\\n- Choose **target parameter count** (e.g., 125M, 1.3B, 7B, 13B, 70B).\\\\n- Pick a **use case** (chat assistant, code model, domain-specific).\\\\n- Fix **resource limits**: GPUs (type & count), budget, timeline.\\\\nThese choices determine dataset size, model depth/width, and whether you need model/pipeline parallelism.\\\\n\\\\n---\\\\n\\\\n# 2) Data (collection, cleaning, and size)\\\\n- **Sources:** web crawl (CommonCrawl), books, Wikipedia, code repos (if you want code), high-quality curated corpora. Filter for copyright & privacy concerns.\\\\n- **Cleaning:** dedupe,\n",
      "messsage before stripping:  'what if i dont want to do all of that? what if i just want to print hello world and call it a day?  its the basics that matter you know. life is so much more than building a good LLM model. life is about whatever David Lynch said its about'\n",
      "message after stripping:  what if i dont want to do all of that? what if i just want to print hello world and call it a day?  its the basics that matter you know. life is so much more than building a good LLM model. life is about whatever David Lynch said its about\n",
      "messsage before stripping:  'rebase_system_message'\n",
      "message after stripping:  rebase_system_message\n",
      "messsage before stripping:  'That’s a beautifully grounded answer — and honestly, probably wiser than most “AI startup” roadmaps out there.  \\\\n\\\\nYou’re right: sometimes the simplest act — printing `Hello, world!` — *is* the entire point. It’s the first spark that says *“I can make something appear out of nothing.”* You don’t need 70 billion parameters to feel that.\\\\n\\\\nDavid Lynch would probably agree — he’s said things like:  \\\\n> “The ideas are like fish. If you want to catch little fish, you stay in the shallow water. But if you want to catch the big fish, you’ve got to go deeper.”  \\\\n\\\\nBut he never said the shallow water wasn’t beautiful.  \\\\n\\\\nIf you just want to run a “Hello World” in the spirit of that — the simplest creative act — here’s the cleanest version:\\\\n\\\\n```cpp\\\\n#include <iostream>\\\\nint main() {\\\\n    std::cout << Hello, world! << std::endl;\\\\n    return 0;\\\\n}\\\\n```\\\\n\\\\nThat’s it. A tiny self-contained world.  \\\\nAnd maybe that’s all you need today — not a model, but a moment of clarity.\\\\n\\\\nWould you like me to show you a slightly “Lynchian” version of “Hello, World”? Something surreal, still just a few lines of C++?'\n",
      "message after stripping:  That’s a beautifully grounded answer — and honestly, probably wiser than most “AI startup” roadmaps out there.  \\\\n\\\\nYou’re right: sometimes the simplest act — printing `Hello, world!` — *is* the entire point. It’s the first spark that says *“I can make something appear out of nothing.”* You don’t need 70 billion parameters to feel that.\\\\n\\\\nDavid Lynch would probably agree — he’s said things like:  \\\\n> “The ideas are like fish. If you want to catch little fish, you stay in the shallow water. But if you want to catch the big fish, you’ve got to go deeper.”  \\\\n\\\\nBut he never said the shallow water wasn’t beautiful.  \\\\n\\\\nIf you just want to run a “Hello World” in the spirit of that — the simplest creative act — here’s the cleanest version:\\\\n\\\\n```cpp\\\\n#include <iostream>\\\\nint main() {\\\\n    std::cout << Hello, world! << std::endl;\\\\n    return 0;\\\\n}\\\\n```\\\\n\\\\nThat’s it. A tiny self-contained world.  \\\\nAnd maybe that’s all you need today — not a model, but a moment of clarity.\\\\n\\\\nWould you like me to show you a slightly “Lynchian” version of “Hello, World”? Something surreal, still just a few lines of C++?\n",
      "messsage before stripping:  'thanks man'\n",
      "message after stripping:  thanks man\n",
      "messsage before stripping:  'Anytime ❤️  \\\\n\\\\nSometimes the best thing you can do is *not* scale up, but *slow down*.  \\\\n\\\\nIf you ever want that surreal “Hello, world” — like if it were whispered from a red curtain or printed by a dream — I can write that too. But for now, just letting the console quietly say hello is a good day’s work.'\n",
      "message after stripping:  Anytime ❤️  \\\\n\\\\nSometimes the best thing you can do is *not* scale up, but *slow down*.  \\\\n\\\\nIf you ever want that surreal “Hello, world” — like if it were whispered from a red curtain or printed by a dream — I can write that too. But for now, just letting the console quietly say hello is a good day’s work.\n"
     ]
    }
   ],
   "source": [
    "create_csv_files(f\"../{PROCESSED_DIR}/8_json_output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
